{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN\n",
    "\n",
    "In ths notebook, we're going to train a simple RNN to do **time-series prediction**. Given some set of input data, it should be able to generate a prediction for the next time step!\n",
    "<img src='assets/time_prediction.png' width=40% />\n",
    "\n",
    "> * First, we'll create our data\n",
    "* Then, define an RNN in PyTorch\n",
    "* Finally, we'll train our network and see how it performs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import resources and create data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21,)\n",
      "(21, 1)\n",
      "x:  [[0.        ]\n",
      " [0.15643447]\n",
      " [0.30901699]\n",
      " [0.4539905 ]\n",
      " [0.58778525]\n",
      " [0.70710678]\n",
      " [0.80901699]\n",
      " [0.89100652]\n",
      " [0.95105652]\n",
      " [0.98768834]\n",
      " [1.        ]\n",
      " [0.98768834]\n",
      " [0.95105652]\n",
      " [0.89100652]\n",
      " [0.80901699]\n",
      " [0.70710678]\n",
      " [0.58778525]\n",
      " [0.4539905 ]\n",
      " [0.30901699]\n",
      " [0.15643447]] y:  [[1.56434465e-01]\n",
      " [3.09016994e-01]\n",
      " [4.53990500e-01]\n",
      " [5.87785252e-01]\n",
      " [7.07106781e-01]\n",
      " [8.09016994e-01]\n",
      " [8.91006524e-01]\n",
      " [9.51056516e-01]\n",
      " [9.87688341e-01]\n",
      " [1.00000000e+00]\n",
      " [9.87688341e-01]\n",
      " [9.51056516e-01]\n",
      " [8.91006524e-01]\n",
      " [8.09016994e-01]\n",
      " [7.07106781e-01]\n",
      " [5.87785252e-01]\n",
      " [4.53990500e-01]\n",
      " [3.09016994e-01]\n",
      " [1.56434465e-01]\n",
      " [1.22464680e-16]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "# how many time steps/data pts are in one batch of data\n",
    "seq_length = 20\n",
    "\n",
    "# generate evenly spaced data pts\n",
    "time_steps = np.linspace(0, np.pi, seq_length + 1) #0 se pi tk 21 numbers generate krega\n",
    "data = np.sin(time_steps) #sin of each num generated\n",
    "print(data.shape)\n",
    "\n",
    "data.resize((seq_length + 1, 1)) # size becomes (seq_length+1, 1), adds an input_size dimension\n",
    "\n",
    "\n",
    "print(data.shape)\n",
    "x = data[:-1] # all but the last piece of data\n",
    "y = data[1:] # all but the first\n",
    "print('x: ',x,'y: ',y)\n",
    "# display the data\n",
    "plt.plot(time_steps[1:], x, 'r.', label='input, x') # x\n",
    "plt.plot(time_steps[1:], y, 'b.', label='target, y') # y\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define the RNN\n",
    "\n",
    "Next, we define an RNN in PyTorch. We'll use `nn.RNN` to create an RNN layer, then we'll add a last, fully-connected layer to get the output size that we want. An RNN takes in a number of parameters:\n",
    "* **input_size** - the size of the input\n",
    "* **hidden_dim** - the number of features in the RNN output and in the hidden state\n",
    "* **n_layers** - the number of layers that make up the RNN, typically 1-3; greater than 1 means that you'll create a stacked RNN\n",
    "* **batch_first** - whether or not the input/output of the RNN will have the batch_size as the first dimension (batch_size, seq_length, hidden_dim)\n",
    "\n",
    "Take a look at the [RNN documentation](https://pytorch.org/docs/stable/nn.html#rnn) to read more about recurrent layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim=hidden_dim\n",
    "\n",
    "        # define an RNN with specified parameters\n",
    "        # batch_first means that the first dim of the input and output will be the batch_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)#hidden dimension is no. of outputs from each h-state\n",
    "        \n",
    "        # last, fully-connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # x (batch_size, seq_length, input_size)\n",
    "        # hidden (n_layers, batch_size, hidden_dim)\n",
    "        # r_out (batch_size, time_step, hidden_size)\n",
    "        #print(x.shape)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # get RNN outputs\n",
    "        print('shape of x: ',x.shape)\n",
    "        print(\"value in x: \",x)\n",
    "        r_out, hidden = self.rnn(x, hidden)\n",
    "        print(\"Value in r_out: \",r_out)#it will give 10 outputs\n",
    "        print(\"r_out shape1: \",r_out.shape)#[1,20 #means 20 times (seq_length) output dega 10 hrr hidden state se,10]\n",
    "        \n",
    "        print(\"value in hidden: \",hidden)#output at each time step for one batch at a time\n",
    "        print(\"hidden shape1: \",hidden.shape) \n",
    "        \n",
    "        # shape output to be (batch_size*seq_length, hidden_dim)\n",
    "        r_out = r_out.view(-1, self.hidden_dim)  \n",
    "        \n",
    "        print(\"r_out shape2(to be send into fc layer): \",r_out.shape)\n",
    "        print(\"r_out after flattening: \",r_out)\n",
    "        \n",
    "        # get final output \n",
    "        output = self.fc(r_out)\n",
    "        print(\"output: \",output)\n",
    "        \n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the input and output dimensions\n",
    "\n",
    "As a check that your model is working as expected, test out how it responds to input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size:  torch.Size([1, 20, 1])\n",
      "shape of x:  torch.Size([1, 20, 1])\n",
      "value in x:  tensor([[[0.0000e+00],\n",
      "         [1.6459e-01],\n",
      "         [3.2470e-01],\n",
      "         [4.7595e-01],\n",
      "         [6.1421e-01],\n",
      "         [7.3572e-01],\n",
      "         [8.3717e-01],\n",
      "         [9.1577e-01],\n",
      "         [9.6940e-01],\n",
      "         [9.9658e-01],\n",
      "         [9.9658e-01],\n",
      "         [9.6940e-01],\n",
      "         [9.1577e-01],\n",
      "         [8.3717e-01],\n",
      "         [7.3572e-01],\n",
      "         [6.1421e-01],\n",
      "         [4.7595e-01],\n",
      "         [3.2470e-01],\n",
      "         [1.6459e-01],\n",
      "         [1.2246e-16]]])\n",
      "Value in r_out:  tensor([[[ 0.4573,  0.3509, -0.2824,  0.2073,  0.3197, -0.2097, -0.3427,\n",
      "          -0.1937,  0.1223, -0.1842],\n",
      "         [ 0.5789,  0.2423, -0.4595,  0.0037,  0.4409, -0.3324,  0.1719,\n",
      "          -0.2063,  0.3354,  0.0430],\n",
      "         [ 0.5097,  0.1280, -0.5547,  0.0615,  0.2338, -0.3349,  0.1501,\n",
      "           0.0490,  0.3318, -0.0945],\n",
      "         [ 0.5173,  0.2054, -0.5171,  0.1417,  0.1718, -0.4353,  0.1337,\n",
      "           0.0112,  0.4427, -0.1498],\n",
      "         [ 0.5330,  0.1818, -0.4956,  0.1524,  0.1493, -0.4807,  0.2254,\n",
      "          -0.0057,  0.5150, -0.1116],\n",
      "         [ 0.5316,  0.1565, -0.4985,  0.1769,  0.0804, -0.4990,  0.2380,\n",
      "           0.0396,  0.5441, -0.1154],\n",
      "         [ 0.5377,  0.1689, -0.4952,  0.1972,  0.0384, -0.5289,  0.2428,\n",
      "           0.0481,  0.5812, -0.1174],\n",
      "         [ 0.5433,  0.1694, -0.4950,  0.2048,  0.0131, -0.5500,  0.2639,\n",
      "           0.0537,  0.6097, -0.1089],\n",
      "         [ 0.5445,  0.1667, -0.4966,  0.2122, -0.0118, -0.5618,  0.2728,\n",
      "           0.0651,  0.6261, -0.1084],\n",
      "         [ 0.5453,  0.1686, -0.4947,  0.2178, -0.0257, -0.5693,  0.2749,\n",
      "           0.0679,  0.6366, -0.1101],\n",
      "         [ 0.5451,  0.1687, -0.4912,  0.2189, -0.0280, -0.5701,  0.2748,\n",
      "           0.0658,  0.6397, -0.1110],\n",
      "         [ 0.5431,  0.1679, -0.4868,  0.2167, -0.0212, -0.5642,  0.2689,\n",
      "           0.0613,  0.6346, -0.1136],\n",
      "         [ 0.5395,  0.1674, -0.4809,  0.2114, -0.0043, -0.5523,  0.2576,\n",
      "           0.0528,  0.6219, -0.1177],\n",
      "         [ 0.5346,  0.1666, -0.4738,  0.2026,  0.0224, -0.5343,  0.2420,\n",
      "           0.0405,  0.6012, -0.1226],\n",
      "         [ 0.5284,  0.1656, -0.4658,  0.1908,  0.0578, -0.5099,  0.2218,\n",
      "           0.0251,  0.5722, -0.1284],\n",
      "         [ 0.5212,  0.1646, -0.4570,  0.1763,  0.1010, -0.4791,  0.1973,\n",
      "           0.0066,  0.5346, -0.1348],\n",
      "         [ 0.5136,  0.1639, -0.4479,  0.1593,  0.1505, -0.4420,  0.1690,\n",
      "          -0.0145,  0.4884, -0.1415],\n",
      "         [ 0.5061,  0.1637, -0.4387,  0.1401,  0.2045, -0.3987,  0.1373,\n",
      "          -0.0378,  0.4339, -0.1482],\n",
      "         [ 0.4991,  0.1644, -0.4299,  0.1192,  0.2611, -0.3498,  0.1027,\n",
      "          -0.0630,  0.3719, -0.1545],\n",
      "         [ 0.4932,  0.1660, -0.4219,  0.0970,  0.3183, -0.2961,  0.0660,\n",
      "          -0.0893,  0.3039, -0.1601]]], grad_fn=<TransposeBackward1>)\n",
      "r_out shape1:  torch.Size([1, 20, 10])\n",
      "value in hidden:  tensor([[[ 0.4932,  0.1660, -0.4219,  0.0970,  0.3183, -0.2961,  0.0660,\n",
      "          -0.0893,  0.3039, -0.1601]]], grad_fn=<StackBackward>)\n",
      "hidden shape1:  torch.Size([1, 1, 10])\n",
      "r_out shape2(to be send into fc layer):  torch.Size([20, 10])\n",
      "r_out after flattening:  tensor([[ 0.4573,  0.3509, -0.2824,  0.2073,  0.3197, -0.2097, -0.3427, -0.1937,\n",
      "          0.1223, -0.1842],\n",
      "        [ 0.5789,  0.2423, -0.4595,  0.0037,  0.4409, -0.3324,  0.1719, -0.2063,\n",
      "          0.3354,  0.0430],\n",
      "        [ 0.5097,  0.1280, -0.5547,  0.0615,  0.2338, -0.3349,  0.1501,  0.0490,\n",
      "          0.3318, -0.0945],\n",
      "        [ 0.5173,  0.2054, -0.5171,  0.1417,  0.1718, -0.4353,  0.1337,  0.0112,\n",
      "          0.4427, -0.1498],\n",
      "        [ 0.5330,  0.1818, -0.4956,  0.1524,  0.1493, -0.4807,  0.2254, -0.0057,\n",
      "          0.5150, -0.1116],\n",
      "        [ 0.5316,  0.1565, -0.4985,  0.1769,  0.0804, -0.4990,  0.2380,  0.0396,\n",
      "          0.5441, -0.1154],\n",
      "        [ 0.5377,  0.1689, -0.4952,  0.1972,  0.0384, -0.5289,  0.2428,  0.0481,\n",
      "          0.5812, -0.1174],\n",
      "        [ 0.5433,  0.1694, -0.4950,  0.2048,  0.0131, -0.5500,  0.2639,  0.0537,\n",
      "          0.6097, -0.1089],\n",
      "        [ 0.5445,  0.1667, -0.4966,  0.2122, -0.0118, -0.5618,  0.2728,  0.0651,\n",
      "          0.6261, -0.1084],\n",
      "        [ 0.5453,  0.1686, -0.4947,  0.2178, -0.0257, -0.5693,  0.2749,  0.0679,\n",
      "          0.6366, -0.1101],\n",
      "        [ 0.5451,  0.1687, -0.4912,  0.2189, -0.0280, -0.5701,  0.2748,  0.0658,\n",
      "          0.6397, -0.1110],\n",
      "        [ 0.5431,  0.1679, -0.4868,  0.2167, -0.0212, -0.5642,  0.2689,  0.0613,\n",
      "          0.6346, -0.1136],\n",
      "        [ 0.5395,  0.1674, -0.4809,  0.2114, -0.0043, -0.5523,  0.2576,  0.0528,\n",
      "          0.6219, -0.1177],\n",
      "        [ 0.5346,  0.1666, -0.4738,  0.2026,  0.0224, -0.5343,  0.2420,  0.0405,\n",
      "          0.6012, -0.1226],\n",
      "        [ 0.5284,  0.1656, -0.4658,  0.1908,  0.0578, -0.5099,  0.2218,  0.0251,\n",
      "          0.5722, -0.1284],\n",
      "        [ 0.5212,  0.1646, -0.4570,  0.1763,  0.1010, -0.4791,  0.1973,  0.0066,\n",
      "          0.5346, -0.1348],\n",
      "        [ 0.5136,  0.1639, -0.4479,  0.1593,  0.1505, -0.4420,  0.1690, -0.0145,\n",
      "          0.4884, -0.1415],\n",
      "        [ 0.5061,  0.1637, -0.4387,  0.1401,  0.2045, -0.3987,  0.1373, -0.0378,\n",
      "          0.4339, -0.1482],\n",
      "        [ 0.4991,  0.1644, -0.4299,  0.1192,  0.2611, -0.3498,  0.1027, -0.0630,\n",
      "          0.3719, -0.1545],\n",
      "        [ 0.4932,  0.1660, -0.4219,  0.0970,  0.3183, -0.2961,  0.0660, -0.0893,\n",
      "          0.3039, -0.1601]], grad_fn=<ViewBackward>)\n",
      "output:  tensor([[0.2478],\n",
      "        [0.4498],\n",
      "        [0.4114],\n",
      "        [0.3728],\n",
      "        [0.3810],\n",
      "        [0.3648],\n",
      "        [0.3507],\n",
      "        [0.3469],\n",
      "        [0.3415],\n",
      "        [0.3370],\n",
      "        [0.3355],\n",
      "        [0.3354],\n",
      "        [0.3371],\n",
      "        [0.3406],\n",
      "        [0.3458],\n",
      "        [0.3524],\n",
      "        [0.3603],\n",
      "        [0.3691],\n",
      "        [0.3783],\n",
      "        [0.3876]], grad_fn=<AddmmBackward>)\n",
      "Output size:  torch.Size([20, 1])\n",
      "Hidden state size:  torch.Size([1, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "# test that dimensions are as expected\n",
    "test_rnn = RNN(input_size=1, output_size=1, hidden_dim=10, n_layers=1) \n",
    "\n",
    "# generate evenly spaced, test data pts\n",
    "time_steps = np.linspace(0, np.pi, seq_length)\n",
    "\n",
    "data = np.sin(time_steps)\n",
    "\n",
    "data.resize((seq_length, 1)) #ek batch mai 2 input honge means batch size=2 hoga and 1 by 1 hum batch feed krenge to the network\n",
    "\n",
    "#here shape of data is (20,2)\n",
    "#print(data)\n",
    "test_input = torch.Tensor(data).unsqueeze(0) # give it a batch_size of 1 as first dimension\n",
    "\n",
    "print('Input size: ', test_input.size())#([1#this 1 is batch size ,20#total data,2#this is input size means 1 batch mai 2 inputs])\n",
    "\n",
    "# test out rnn sizes\n",
    "test_out, test_h = test_rnn(test_input, None)#calling the forward function #None is initial hidden state\n",
    "\n",
    "print('Output size: ', test_out.size())\n",
    "print('Hidden state size: ', test_h.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training the RNN\n",
    "\n",
    "Next, we'll instantiate an RNN with some specified hyperparameters. Then train it over a series of steps, and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(1, 32, batch_first=True)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# decide on hyperparameters\n",
    "input_size=1 \n",
    "output_size=1\n",
    "hidden_dim=32\n",
    "n_layers=1\n",
    "\n",
    "# instantiate an RNN\n",
    "rnn = RNN(input_size, output_size, hidden_dim, n_layers)\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Optimization\n",
    "\n",
    "This is a regression problem: can we train an RNN to accurately predict the next data point, given a current data point?\n",
    "\n",
    ">* The data points are coordinate values, so to compare a predicted and ground_truth point, we'll use a regression loss: the mean squared error.\n",
    "* It's typical to use an Adam optimizer for recurrent models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss and Adam optimizer with a learning rate of 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.01) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the training function\n",
    "\n",
    "This function takes in an rnn, a number of steps to train for, and returns a trained rnn. This function is also responsible for displaying the loss and the predictions, every so often.\n",
    "\n",
    "#### Hidden State\n",
    "\n",
    "Pay close attention to the hidden state, here:\n",
    "* Before looping over a batch of training data, the hidden state is initialized\n",
    "* After a new hidden state is generated by the rnn, we get the latest hidden state, and use that as input to the rnn for the following steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the RNN\n",
    "def train(rnn, n_steps, print_every):\n",
    "    \n",
    "    # initialize the hidden state\n",
    "    hidden = None      \n",
    "    \n",
    "    for batch_i, step in enumerate(range(n_steps)):\n",
    "        # defining the training data \n",
    "        time_steps = np.linspace(step * np.pi, (step+1)*np.pi, seq_length + 1)\n",
    "        data = np.sin(time_steps)\n",
    "        data.resize((seq_length + 1, 1)) # input_size=1\n",
    "\n",
    "        x = data[:-1]\n",
    "        y = data[1:]\n",
    "        \n",
    "        # convert data into Tensors\n",
    "        x_tensor = torch.Tensor(x).unsqueeze(0) # unsqueeze gives a 1, batch_size dimension\n",
    "        y_tensor = torch.Tensor(y)\n",
    "\n",
    "        # outputs from the rnn\n",
    "        prediction, hidden = rnn(x_tensor, hidden)\n",
    "\n",
    "        ## Representing Memory ##\n",
    "        # make a new variable for hidden and detach the hidden state from its history\n",
    "        # this way, we don't backpropagate through the entire history\n",
    "        hidden = hidden.data\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(prediction, y_tensor)\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # perform backprop and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # display loss and predictions\n",
    "        if batch_i%print_every == 0:        \n",
    "            print('Loss: ', loss.item())\n",
    "            plt.plot(time_steps[1:], x, 'r.') # input\n",
    "            plt.plot(time_steps[1:], prediction.data.numpy().flatten(), 'b.') # predictions\n",
    "            plt.show()\n",
    "    \n",
    "    return rnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x:  torch.Size([1, 20, 1])\n",
      "value in x:  tensor([[[0.0000],\n",
      "         [0.1564],\n",
      "         [0.3090],\n",
      "         [0.4540],\n",
      "         [0.5878],\n",
      "         [0.7071],\n",
      "         [0.8090],\n",
      "         [0.8910],\n",
      "         [0.9511],\n",
      "         [0.9877],\n",
      "         [1.0000],\n",
      "         [0.9877],\n",
      "         [0.9511],\n",
      "         [0.8910],\n",
      "         [0.8090],\n",
      "         [0.7071],\n",
      "         [0.5878],\n",
      "         [0.4540],\n",
      "         [0.3090],\n",
      "         [0.1564]]])\n",
      "Value in r_out:  tensor([[[ 1.5071e-01, -6.3308e-02, -2.8243e-02,  7.1289e-02, -1.0563e-02,\n",
      "          -1.0289e-01, -2.8182e-02, -8.9889e-02,  4.2043e-02,  1.7839e-02,\n",
      "          -1.2777e-01,  7.1868e-02, -6.0984e-02, -1.5628e-01, -1.1333e-01,\n",
      "          -4.3031e-02, -2.5536e-01, -1.9966e-02, -4.8298e-02, -9.2757e-03,\n",
      "          -3.8018e-02, -3.5107e-03, -4.4264e-02,  2.4988e-02, -1.2654e-01,\n",
      "          -6.9016e-02,  1.8495e-01, -2.1023e-01,  1.0803e-01,  1.5212e-01,\n",
      "           3.5596e-02, -1.3051e-02],\n",
      "         [ 1.1885e-01, -3.6658e-02, -4.9107e-02, -1.3752e-02, -1.0212e-01,\n",
      "          -2.0001e-02,  9.9828e-02, -4.1453e-02, -2.3101e-02, -8.4665e-02,\n",
      "          -1.7544e-01, -1.7202e-02, -3.7556e-02, -1.8624e-01, -5.5604e-02,\n",
      "          -1.7190e-03, -1.7680e-01, -1.1606e-01, -3.3175e-03, -8.7122e-03,\n",
      "           3.6178e-02,  1.0195e-01, -2.7412e-02, -5.5597e-02, -1.4870e-02,\n",
      "          -6.8390e-02,  2.0669e-01, -2.1445e-01,  1.1523e-01,  2.1319e-01,\n",
      "           1.1443e-01,  3.1506e-02],\n",
      "         [-2.7547e-03, -5.0179e-03, -3.0030e-02, -8.3208e-02, -1.5595e-01,\n",
      "          -4.8766e-02,  1.6676e-01, -1.2981e-01,  3.8667e-02, -1.8775e-01,\n",
      "          -2.3012e-01, -1.0259e-01, -1.1750e-01, -1.6159e-01,  4.5501e-03,\n",
      "           2.1482e-02, -2.2466e-01, -1.6400e-01, -7.8937e-02, -6.9123e-02,\n",
      "           8.5435e-02,  2.2528e-01,  2.9273e-05, -1.4513e-01,  5.4644e-02,\n",
      "          -1.7520e-01,  2.9020e-01, -1.3332e-01,  1.4453e-01,  2.0631e-01,\n",
      "          -2.0261e-02,  8.0307e-02],\n",
      "         [-1.0186e-01,  4.4211e-02, -7.2599e-02, -1.3715e-01, -1.9304e-01,\n",
      "           1.6675e-02,  1.1791e-01, -1.9776e-01,  6.5801e-02, -3.1444e-01,\n",
      "          -3.0291e-01, -9.8707e-02, -1.4885e-01, -9.7002e-02,  1.0799e-01,\n",
      "          -1.0180e-02, -2.6690e-01, -2.3246e-01, -1.0434e-01, -9.7334e-02,\n",
      "           1.5926e-01,  2.4153e-01,  5.9477e-02, -2.5235e-01,  1.0208e-01,\n",
      "          -3.0696e-01,  3.6412e-01, -1.1523e-01,  1.8237e-01,  2.9364e-01,\n",
      "          -9.1254e-02,  1.9568e-01],\n",
      "         [-1.6997e-01,  9.4517e-02, -1.4563e-01, -1.4171e-01, -2.2712e-01,\n",
      "           5.1390e-02,  9.3805e-02, -2.4097e-01,  7.3026e-02, -3.8967e-01,\n",
      "          -3.3698e-01, -1.1891e-01, -1.7937e-01, -3.9998e-02,  1.8902e-01,\n",
      "          -9.0603e-02, -2.9410e-01, -2.8429e-01, -1.1029e-01, -6.8449e-02,\n",
      "           2.2489e-01,  2.0704e-01,  1.3660e-01, -3.3181e-01,  1.4297e-01,\n",
      "          -3.7113e-01,  4.1214e-01, -1.3024e-01,  2.1600e-01,  3.1891e-01,\n",
      "          -1.5718e-01,  2.5103e-01],\n",
      "         [-1.9207e-01,  8.3488e-02, -2.2105e-01, -1.2300e-01, -2.0600e-01,\n",
      "           8.2471e-02,  7.5649e-02, -2.7092e-01,  8.2183e-02, -4.0355e-01,\n",
      "          -3.3002e-01, -8.6732e-02, -1.8392e-01, -4.7288e-02,  2.3724e-01,\n",
      "          -1.5696e-01, -2.8308e-01, -2.9044e-01, -9.0396e-02, -2.2770e-02,\n",
      "           2.6340e-01,  1.7308e-01,  1.4458e-01, -3.5860e-01,  1.4374e-01,\n",
      "          -3.8459e-01,  4.3875e-01, -1.6392e-01,  2.0398e-01,  3.0233e-01,\n",
      "          -1.8574e-01,  2.7618e-01],\n",
      "         [-1.9634e-01,  8.0259e-02, -2.2522e-01, -1.3575e-01, -2.0133e-01,\n",
      "           7.7563e-02,  6.6427e-02, -2.6815e-01,  1.1000e-01, -3.8469e-01,\n",
      "          -3.1598e-01, -7.9835e-02, -1.7179e-01, -5.9900e-02,  2.5651e-01,\n",
      "          -1.6853e-01, -2.8441e-01, -3.0608e-01, -7.6921e-02,  1.9275e-04,\n",
      "           2.7394e-01,  1.5773e-01,  1.2688e-01, -3.4920e-01,  1.5471e-01,\n",
      "          -3.8015e-01,  4.5602e-01, -1.7395e-01,  1.8722e-01,  2.8897e-01,\n",
      "          -1.8373e-01,  2.9773e-01],\n",
      "         [-2.0443e-01,  8.8328e-02, -1.9391e-01, -1.7170e-01, -2.1025e-01,\n",
      "           5.8671e-02,  6.4898e-02, -2.7450e-01,  1.2787e-01, -4.0250e-01,\n",
      "          -3.3906e-01, -9.8689e-02, -1.7980e-01, -5.8553e-02,  2.6565e-01,\n",
      "          -1.4458e-01, -2.9691e-01, -3.3618e-01, -9.5051e-02, -1.0252e-02,\n",
      "           2.6941e-01,  1.7874e-01,  1.4012e-01, -3.6243e-01,  1.7093e-01,\n",
      "          -3.9933e-01,  4.8074e-01, -1.4456e-01,  1.8498e-01,  3.1093e-01,\n",
      "          -1.7506e-01,  3.1415e-01],\n",
      "         [-2.3051e-01,  1.0990e-01, -1.8280e-01, -1.9751e-01, -2.3433e-01,\n",
      "           6.2795e-02,  7.7030e-02, -2.9612e-01,  1.3568e-01, -4.5191e-01,\n",
      "          -3.7849e-01, -1.2920e-01, -2.0250e-01, -3.9732e-02,  2.9582e-01,\n",
      "          -1.2990e-01, -3.1097e-01, -3.6467e-01, -1.2637e-01, -3.1877e-02,\n",
      "           2.8272e-01,  2.0767e-01,  1.7210e-01, -4.0096e-01,  1.9887e-01,\n",
      "          -4.4413e-01,  5.0963e-01, -1.1471e-01,  2.1172e-01,  3.4901e-01,\n",
      "          -1.8999e-01,  3.3937e-01],\n",
      "         [-2.6289e-01,  1.2829e-01, -2.0856e-01, -2.0325e-01, -2.5403e-01,\n",
      "           8.5145e-02,  7.9226e-02, -3.2073e-01,  1.3859e-01, -4.9289e-01,\n",
      "          -4.0213e-01, -1.4221e-01, -2.2057e-01, -1.9425e-02,  3.3120e-01,\n",
      "          -1.5125e-01, -3.2399e-01, -3.7870e-01, -1.4051e-01, -3.7646e-02,\n",
      "           3.1130e-01,  2.1659e-01,  1.9940e-01, -4.3851e-01,  2.1694e-01,\n",
      "          -4.8058e-01,  5.2852e-01, -1.1230e-01,  2.3284e-01,  3.6836e-01,\n",
      "          -2.2197e-01,  3.6585e-01],\n",
      "         [-2.7840e-01,  1.3134e-01, -2.4625e-01, -1.9090e-01, -2.5470e-01,\n",
      "           1.0604e-01,  6.5239e-02, -3.3192e-01,  1.3741e-01, -5.0005e-01,\n",
      "          -3.9806e-01, -1.2938e-01, -2.2335e-01, -1.0757e-02,  3.5056e-01,\n",
      "          -1.9200e-01, -3.2481e-01, -3.7972e-01, -1.2755e-01, -2.2706e-02,\n",
      "           3.3111e-01,  1.9706e-01,  2.0860e-01, -4.5015e-01,  2.1667e-01,\n",
      "          -4.8617e-01,  5.3064e-01, -1.3648e-01,  2.3147e-01,  3.5825e-01,\n",
      "          -2.4459e-01,  3.7589e-01],\n",
      "         [-2.6877e-01,  1.2068e-01, -2.6961e-01, -1.6972e-01, -2.3707e-01,\n",
      "           1.1129e-01,  4.6970e-02, -3.2552e-01,  1.3074e-01, -4.7509e-01,\n",
      "          -3.7358e-01, -1.0344e-01, -2.1125e-01, -1.6558e-02,  3.4510e-01,\n",
      "          -2.2220e-01, -3.1279e-01, -3.6998e-01, -1.0243e-01,  1.8672e-03,\n",
      "           3.3047e-01,  1.6718e-01,  1.9853e-01, -4.3317e-01,  2.0078e-01,\n",
      "          -4.6302e-01,  5.1913e-01, -1.6649e-01,  2.1421e-01,  3.3067e-01,\n",
      "          -2.4401e-01,  3.6594e-01],\n",
      "         [-2.4200e-01,  1.0504e-01, -2.7006e-01, -1.5138e-01, -2.1517e-01,\n",
      "           1.0203e-01,  3.6643e-02, -3.0738e-01,  1.2008e-01, -4.3535e-01,\n",
      "          -3.4406e-01, -8.1642e-02, -1.9239e-01, -3.0012e-02,  3.2153e-01,\n",
      "          -2.2879e-01, -2.9582e-01, -3.5365e-01, -8.0984e-02,  1.9558e-02,\n",
      "           3.1285e-01,  1.4413e-01,  1.7799e-01, -4.0064e-01,  1.8019e-01,\n",
      "          -4.2683e-01,  5.0056e-01, -1.8463e-01,  1.9524e-01,  3.0183e-01,\n",
      "          -2.2531e-01,  3.4384e-01],\n",
      "         [-2.1038e-01,  9.0957e-02, -2.5590e-01, -1.3696e-01, -1.9861e-01,\n",
      "           8.7525e-02,  3.4607e-02, -2.8662e-01,  1.0623e-01, -3.9769e-01,\n",
      "          -3.1983e-01, -6.9370e-02, -1.7455e-01, -4.2311e-02,  2.9021e-01,\n",
      "          -2.2023e-01, -2.8090e-01, -3.3461e-01, -6.8767e-02,  2.4366e-02,\n",
      "           2.8787e-01,  1.3318e-01,  1.5797e-01, -3.6670e-01,  1.6081e-01,\n",
      "          -3.9119e-01,  4.7903e-01, -1.9043e-01,  1.8220e-01,  2.7970e-01,\n",
      "          -2.0185e-01,  3.1744e-01],\n",
      "         [-1.8002e-01,  7.8672e-02, -2.4271e-01, -1.1940e-01, -1.8661e-01,\n",
      "           7.6803e-02,  3.4983e-02, -2.6702e-01,  8.7747e-02, -3.6514e-01,\n",
      "          -2.9883e-01, -5.9885e-02, -1.5899e-01, -5.0552e-02,  2.5747e-01,\n",
      "          -2.1208e-01, -2.6729e-01, -3.1196e-01, -5.9474e-02,  2.2800e-02,\n",
      "           2.6334e-01,  1.2594e-01,  1.4157e-01, -3.3562e-01,  1.4143e-01,\n",
      "          -3.5802e-01,  4.5373e-01, -1.9546e-01,  1.7440e-01,  2.6048e-01,\n",
      "          -1.8194e-01,  2.8908e-01],\n",
      "         [-1.4785e-01,  6.3772e-02, -2.3844e-01, -9.2331e-02, -1.7187e-01,\n",
      "           7.0945e-02,  3.2614e-02, -2.4580e-01,  6.4265e-02, -3.2678e-01,\n",
      "          -2.7136e-01, -4.3877e-02, -1.4124e-01, -5.8804e-02,  2.2127e-01,\n",
      "          -2.1277e-01, -2.5064e-01, -2.8200e-01, -4.3933e-02,  2.4801e-02,\n",
      "           2.3981e-01,  1.1192e-01,  1.2305e-01, -3.0044e-01,  1.1670e-01,\n",
      "          -3.1823e-01,  4.2078e-01, -2.0966e-01,  1.6492e-01,  2.3423e-01,\n",
      "          -1.6452e-01,  2.5568e-01],\n",
      "         [-1.0680e-01,  4.2105e-02, -2.3775e-01, -5.5984e-02, -1.4872e-01,\n",
      "           6.4202e-02,  2.6311e-02, -2.1770e-01,  3.7429e-02, -2.7018e-01,\n",
      "          -2.3066e-01, -1.7853e-02, -1.1669e-01, -7.2762e-02,  1.7516e-01,\n",
      "          -2.1782e-01, -2.2809e-01, -2.4281e-01, -1.8777e-02,  3.3917e-02,\n",
      "           2.1128e-01,  8.8256e-02,  9.5274e-02, -2.5155e-01,  8.2956e-02,\n",
      "          -2.6197e-01,  3.7687e-01, -2.3289e-01,  1.4751e-01,  1.9555e-01,\n",
      "          -1.4203e-01,  2.1267e-01],\n",
      "         [-5.3292e-02,  1.3715e-02, -2.2922e-01, -1.5922e-02, -1.1787e-01,\n",
      "           5.0339e-02,  1.9010e-02, -1.8034e-01,  9.5474e-03, -1.9279e-01,\n",
      "          -1.7899e-01,  1.3732e-02, -8.5158e-02, -9.4412e-02,  1.1516e-01,\n",
      "          -2.1596e-01, -2.0078e-01, -1.9619e-01,  1.1215e-02,  4.5648e-02,\n",
      "           1.7185e-01,  6.1060e-02,  5.6761e-02, -1.8595e-01,  4.1429e-02,\n",
      "          -1.8790e-01,  3.2214e-01, -2.5699e-01,  1.2203e-01,  1.4838e-01,\n",
      "          -1.0818e-01,  1.5839e-01],\n",
      "         [ 9.2062e-03, -1.7591e-02, -2.0626e-01,  2.1776e-02, -8.5059e-02,\n",
      "           2.8256e-02,  1.4918e-02, -1.3632e-01, -1.7795e-02, -1.0402e-01,\n",
      "          -1.2504e-01,  4.3299e-02, -5.0928e-02, -1.2112e-01,  4.3994e-02,\n",
      "          -1.9952e-01, -1.7261e-01, -1.4612e-01,  3.8158e-02,  5.2772e-02,\n",
      "           1.2136e-01,  3.9105e-02,  1.2276e-02, -1.0997e-01, -3.0467e-03,\n",
      "          -1.0457e-01,  2.6011e-01, -2.7406e-01,  9.3755e-02,  1.0193e-01,\n",
      "          -6.3345e-02,  9.5803e-02],\n",
      "         [ 7.2797e-02, -4.7325e-02, -1.7188e-01,  5.4540e-02, -5.6092e-02,\n",
      "           2.2873e-03,  1.6044e-02, -9.1148e-02, -4.4141e-02, -1.7708e-02,\n",
      "          -7.6761e-02,  6.6334e-02, -1.9239e-02, -1.4807e-01, -3.0697e-02,\n",
      "          -1.7020e-01, -1.4716e-01, -9.6117e-02,  5.7777e-02,  5.1680e-02,\n",
      "           6.5743e-02,  2.6381e-02, -3.1272e-02, -3.4273e-02, -4.5746e-02,\n",
      "          -2.3636e-02,  1.9548e-01, -2.8273e-01,  6.8662e-02,  6.2796e-02,\n",
      "          -1.4054e-02,  3.0684e-02]]], grad_fn=<TransposeBackward1>)\n",
      "r_out shape1:  torch.Size([1, 20, 32])\n",
      "value in hidden:  tensor([[[ 0.0728, -0.0473, -0.1719,  0.0545, -0.0561,  0.0023,  0.0160,\n",
      "          -0.0911, -0.0441, -0.0177, -0.0768,  0.0663, -0.0192, -0.1481,\n",
      "          -0.0307, -0.1702, -0.1472, -0.0961,  0.0578,  0.0517,  0.0657,\n",
      "           0.0264, -0.0313, -0.0343, -0.0457, -0.0236,  0.1955, -0.2827,\n",
      "           0.0687,  0.0628, -0.0141,  0.0307]]], grad_fn=<StackBackward>)\n",
      "hidden shape1:  torch.Size([1, 1, 32])\n",
      "r_out shape2(to be send into fc layer):  torch.Size([20, 32])\n",
      "r_out after flattening:  tensor([[ 1.5071e-01, -6.3308e-02, -2.8243e-02,  7.1289e-02, -1.0563e-02,\n",
      "         -1.0289e-01, -2.8182e-02, -8.9889e-02,  4.2043e-02,  1.7839e-02,\n",
      "         -1.2777e-01,  7.1868e-02, -6.0984e-02, -1.5628e-01, -1.1333e-01,\n",
      "         -4.3031e-02, -2.5536e-01, -1.9966e-02, -4.8298e-02, -9.2757e-03,\n",
      "         -3.8018e-02, -3.5107e-03, -4.4264e-02,  2.4988e-02, -1.2654e-01,\n",
      "         -6.9016e-02,  1.8495e-01, -2.1023e-01,  1.0803e-01,  1.5212e-01,\n",
      "          3.5596e-02, -1.3051e-02],\n",
      "        [ 1.1885e-01, -3.6658e-02, -4.9107e-02, -1.3752e-02, -1.0212e-01,\n",
      "         -2.0001e-02,  9.9828e-02, -4.1453e-02, -2.3101e-02, -8.4665e-02,\n",
      "         -1.7544e-01, -1.7202e-02, -3.7556e-02, -1.8624e-01, -5.5604e-02,\n",
      "         -1.7190e-03, -1.7680e-01, -1.1606e-01, -3.3175e-03, -8.7122e-03,\n",
      "          3.6178e-02,  1.0195e-01, -2.7412e-02, -5.5597e-02, -1.4870e-02,\n",
      "         -6.8390e-02,  2.0669e-01, -2.1445e-01,  1.1523e-01,  2.1319e-01,\n",
      "          1.1443e-01,  3.1506e-02],\n",
      "        [-2.7547e-03, -5.0179e-03, -3.0030e-02, -8.3208e-02, -1.5595e-01,\n",
      "         -4.8766e-02,  1.6676e-01, -1.2981e-01,  3.8667e-02, -1.8775e-01,\n",
      "         -2.3012e-01, -1.0259e-01, -1.1750e-01, -1.6159e-01,  4.5501e-03,\n",
      "          2.1482e-02, -2.2466e-01, -1.6400e-01, -7.8937e-02, -6.9123e-02,\n",
      "          8.5435e-02,  2.2528e-01,  2.9273e-05, -1.4513e-01,  5.4644e-02,\n",
      "         -1.7520e-01,  2.9020e-01, -1.3332e-01,  1.4453e-01,  2.0631e-01,\n",
      "         -2.0261e-02,  8.0307e-02],\n",
      "        [-1.0186e-01,  4.4211e-02, -7.2599e-02, -1.3715e-01, -1.9304e-01,\n",
      "          1.6675e-02,  1.1791e-01, -1.9776e-01,  6.5801e-02, -3.1444e-01,\n",
      "         -3.0291e-01, -9.8707e-02, -1.4885e-01, -9.7002e-02,  1.0799e-01,\n",
      "         -1.0180e-02, -2.6690e-01, -2.3246e-01, -1.0434e-01, -9.7334e-02,\n",
      "          1.5926e-01,  2.4153e-01,  5.9477e-02, -2.5235e-01,  1.0208e-01,\n",
      "         -3.0696e-01,  3.6412e-01, -1.1523e-01,  1.8237e-01,  2.9364e-01,\n",
      "         -9.1254e-02,  1.9568e-01],\n",
      "        [-1.6997e-01,  9.4517e-02, -1.4563e-01, -1.4171e-01, -2.2712e-01,\n",
      "          5.1390e-02,  9.3805e-02, -2.4097e-01,  7.3026e-02, -3.8967e-01,\n",
      "         -3.3698e-01, -1.1891e-01, -1.7937e-01, -3.9998e-02,  1.8902e-01,\n",
      "         -9.0603e-02, -2.9410e-01, -2.8429e-01, -1.1029e-01, -6.8449e-02,\n",
      "          2.2489e-01,  2.0704e-01,  1.3660e-01, -3.3181e-01,  1.4297e-01,\n",
      "         -3.7113e-01,  4.1214e-01, -1.3024e-01,  2.1600e-01,  3.1891e-01,\n",
      "         -1.5718e-01,  2.5103e-01],\n",
      "        [-1.9207e-01,  8.3488e-02, -2.2105e-01, -1.2300e-01, -2.0600e-01,\n",
      "          8.2471e-02,  7.5649e-02, -2.7092e-01,  8.2183e-02, -4.0355e-01,\n",
      "         -3.3002e-01, -8.6732e-02, -1.8392e-01, -4.7288e-02,  2.3724e-01,\n",
      "         -1.5696e-01, -2.8308e-01, -2.9044e-01, -9.0396e-02, -2.2770e-02,\n",
      "          2.6340e-01,  1.7308e-01,  1.4458e-01, -3.5860e-01,  1.4374e-01,\n",
      "         -3.8459e-01,  4.3875e-01, -1.6392e-01,  2.0398e-01,  3.0233e-01,\n",
      "         -1.8574e-01,  2.7618e-01],\n",
      "        [-1.9634e-01,  8.0259e-02, -2.2522e-01, -1.3575e-01, -2.0133e-01,\n",
      "          7.7563e-02,  6.6427e-02, -2.6815e-01,  1.1000e-01, -3.8469e-01,\n",
      "         -3.1598e-01, -7.9835e-02, -1.7179e-01, -5.9900e-02,  2.5651e-01,\n",
      "         -1.6853e-01, -2.8441e-01, -3.0608e-01, -7.6921e-02,  1.9275e-04,\n",
      "          2.7394e-01,  1.5773e-01,  1.2688e-01, -3.4920e-01,  1.5471e-01,\n",
      "         -3.8015e-01,  4.5602e-01, -1.7395e-01,  1.8722e-01,  2.8897e-01,\n",
      "         -1.8373e-01,  2.9773e-01],\n",
      "        [-2.0443e-01,  8.8328e-02, -1.9391e-01, -1.7170e-01, -2.1025e-01,\n",
      "          5.8671e-02,  6.4898e-02, -2.7450e-01,  1.2787e-01, -4.0250e-01,\n",
      "         -3.3906e-01, -9.8689e-02, -1.7980e-01, -5.8553e-02,  2.6565e-01,\n",
      "         -1.4458e-01, -2.9691e-01, -3.3618e-01, -9.5051e-02, -1.0252e-02,\n",
      "          2.6941e-01,  1.7874e-01,  1.4012e-01, -3.6243e-01,  1.7093e-01,\n",
      "         -3.9933e-01,  4.8074e-01, -1.4456e-01,  1.8498e-01,  3.1093e-01,\n",
      "         -1.7506e-01,  3.1415e-01],\n",
      "        [-2.3051e-01,  1.0990e-01, -1.8280e-01, -1.9751e-01, -2.3433e-01,\n",
      "          6.2795e-02,  7.7030e-02, -2.9612e-01,  1.3568e-01, -4.5191e-01,\n",
      "         -3.7849e-01, -1.2920e-01, -2.0250e-01, -3.9732e-02,  2.9582e-01,\n",
      "         -1.2990e-01, -3.1097e-01, -3.6467e-01, -1.2637e-01, -3.1877e-02,\n",
      "          2.8272e-01,  2.0767e-01,  1.7210e-01, -4.0096e-01,  1.9887e-01,\n",
      "         -4.4413e-01,  5.0963e-01, -1.1471e-01,  2.1172e-01,  3.4901e-01,\n",
      "         -1.8999e-01,  3.3937e-01],\n",
      "        [-2.6289e-01,  1.2829e-01, -2.0856e-01, -2.0325e-01, -2.5403e-01,\n",
      "          8.5145e-02,  7.9226e-02, -3.2073e-01,  1.3859e-01, -4.9289e-01,\n",
      "         -4.0213e-01, -1.4221e-01, -2.2057e-01, -1.9425e-02,  3.3120e-01,\n",
      "         -1.5125e-01, -3.2399e-01, -3.7870e-01, -1.4051e-01, -3.7646e-02,\n",
      "          3.1130e-01,  2.1659e-01,  1.9940e-01, -4.3851e-01,  2.1694e-01,\n",
      "         -4.8058e-01,  5.2852e-01, -1.1230e-01,  2.3284e-01,  3.6836e-01,\n",
      "         -2.2197e-01,  3.6585e-01],\n",
      "        [-2.7840e-01,  1.3134e-01, -2.4625e-01, -1.9090e-01, -2.5470e-01,\n",
      "          1.0604e-01,  6.5239e-02, -3.3192e-01,  1.3741e-01, -5.0005e-01,\n",
      "         -3.9806e-01, -1.2938e-01, -2.2335e-01, -1.0757e-02,  3.5056e-01,\n",
      "         -1.9200e-01, -3.2481e-01, -3.7972e-01, -1.2755e-01, -2.2706e-02,\n",
      "          3.3111e-01,  1.9706e-01,  2.0860e-01, -4.5015e-01,  2.1667e-01,\n",
      "         -4.8617e-01,  5.3064e-01, -1.3648e-01,  2.3147e-01,  3.5825e-01,\n",
      "         -2.4459e-01,  3.7589e-01],\n",
      "        [-2.6877e-01,  1.2068e-01, -2.6961e-01, -1.6972e-01, -2.3707e-01,\n",
      "          1.1129e-01,  4.6970e-02, -3.2552e-01,  1.3074e-01, -4.7509e-01,\n",
      "         -3.7358e-01, -1.0344e-01, -2.1125e-01, -1.6558e-02,  3.4510e-01,\n",
      "         -2.2220e-01, -3.1279e-01, -3.6998e-01, -1.0243e-01,  1.8672e-03,\n",
      "          3.3047e-01,  1.6718e-01,  1.9853e-01, -4.3317e-01,  2.0078e-01,\n",
      "         -4.6302e-01,  5.1913e-01, -1.6649e-01,  2.1421e-01,  3.3067e-01,\n",
      "         -2.4401e-01,  3.6594e-01],\n",
      "        [-2.4200e-01,  1.0504e-01, -2.7006e-01, -1.5138e-01, -2.1517e-01,\n",
      "          1.0203e-01,  3.6643e-02, -3.0738e-01,  1.2008e-01, -4.3535e-01,\n",
      "         -3.4406e-01, -8.1642e-02, -1.9239e-01, -3.0012e-02,  3.2153e-01,\n",
      "         -2.2879e-01, -2.9582e-01, -3.5365e-01, -8.0984e-02,  1.9558e-02,\n",
      "          3.1285e-01,  1.4413e-01,  1.7799e-01, -4.0064e-01,  1.8019e-01,\n",
      "         -4.2683e-01,  5.0056e-01, -1.8463e-01,  1.9524e-01,  3.0183e-01,\n",
      "         -2.2531e-01,  3.4384e-01],\n",
      "        [-2.1038e-01,  9.0957e-02, -2.5590e-01, -1.3696e-01, -1.9861e-01,\n",
      "          8.7525e-02,  3.4607e-02, -2.8662e-01,  1.0623e-01, -3.9769e-01,\n",
      "         -3.1983e-01, -6.9370e-02, -1.7455e-01, -4.2311e-02,  2.9021e-01,\n",
      "         -2.2023e-01, -2.8090e-01, -3.3461e-01, -6.8767e-02,  2.4366e-02,\n",
      "          2.8787e-01,  1.3318e-01,  1.5797e-01, -3.6670e-01,  1.6081e-01,\n",
      "         -3.9119e-01,  4.7903e-01, -1.9043e-01,  1.8220e-01,  2.7970e-01,\n",
      "         -2.0185e-01,  3.1744e-01],\n",
      "        [-1.8002e-01,  7.8672e-02, -2.4271e-01, -1.1940e-01, -1.8661e-01,\n",
      "          7.6803e-02,  3.4983e-02, -2.6702e-01,  8.7747e-02, -3.6514e-01,\n",
      "         -2.9883e-01, -5.9885e-02, -1.5899e-01, -5.0552e-02,  2.5747e-01,\n",
      "         -2.1208e-01, -2.6729e-01, -3.1196e-01, -5.9474e-02,  2.2800e-02,\n",
      "          2.6334e-01,  1.2594e-01,  1.4157e-01, -3.3562e-01,  1.4143e-01,\n",
      "         -3.5802e-01,  4.5373e-01, -1.9546e-01,  1.7440e-01,  2.6048e-01,\n",
      "         -1.8194e-01,  2.8908e-01],\n",
      "        [-1.4785e-01,  6.3772e-02, -2.3844e-01, -9.2331e-02, -1.7187e-01,\n",
      "          7.0945e-02,  3.2614e-02, -2.4580e-01,  6.4265e-02, -3.2678e-01,\n",
      "         -2.7136e-01, -4.3877e-02, -1.4124e-01, -5.8804e-02,  2.2127e-01,\n",
      "         -2.1277e-01, -2.5064e-01, -2.8200e-01, -4.3933e-02,  2.4801e-02,\n",
      "          2.3981e-01,  1.1192e-01,  1.2305e-01, -3.0044e-01,  1.1670e-01,\n",
      "         -3.1823e-01,  4.2078e-01, -2.0966e-01,  1.6492e-01,  2.3423e-01,\n",
      "         -1.6452e-01,  2.5568e-01],\n",
      "        [-1.0680e-01,  4.2105e-02, -2.3775e-01, -5.5984e-02, -1.4872e-01,\n",
      "          6.4202e-02,  2.6311e-02, -2.1770e-01,  3.7429e-02, -2.7018e-01,\n",
      "         -2.3066e-01, -1.7853e-02, -1.1669e-01, -7.2762e-02,  1.7516e-01,\n",
      "         -2.1782e-01, -2.2809e-01, -2.4281e-01, -1.8777e-02,  3.3917e-02,\n",
      "          2.1128e-01,  8.8256e-02,  9.5274e-02, -2.5155e-01,  8.2956e-02,\n",
      "         -2.6197e-01,  3.7687e-01, -2.3289e-01,  1.4751e-01,  1.9555e-01,\n",
      "         -1.4203e-01,  2.1267e-01],\n",
      "        [-5.3292e-02,  1.3715e-02, -2.2922e-01, -1.5922e-02, -1.1787e-01,\n",
      "          5.0339e-02,  1.9010e-02, -1.8034e-01,  9.5474e-03, -1.9279e-01,\n",
      "         -1.7899e-01,  1.3732e-02, -8.5158e-02, -9.4412e-02,  1.1516e-01,\n",
      "         -2.1596e-01, -2.0078e-01, -1.9619e-01,  1.1215e-02,  4.5648e-02,\n",
      "          1.7185e-01,  6.1060e-02,  5.6761e-02, -1.8595e-01,  4.1429e-02,\n",
      "         -1.8790e-01,  3.2214e-01, -2.5699e-01,  1.2203e-01,  1.4838e-01,\n",
      "         -1.0818e-01,  1.5839e-01],\n",
      "        [ 9.2062e-03, -1.7591e-02, -2.0626e-01,  2.1776e-02, -8.5059e-02,\n",
      "          2.8256e-02,  1.4918e-02, -1.3632e-01, -1.7795e-02, -1.0402e-01,\n",
      "         -1.2504e-01,  4.3299e-02, -5.0928e-02, -1.2112e-01,  4.3994e-02,\n",
      "         -1.9952e-01, -1.7261e-01, -1.4612e-01,  3.8158e-02,  5.2772e-02,\n",
      "          1.2136e-01,  3.9105e-02,  1.2276e-02, -1.0997e-01, -3.0467e-03,\n",
      "         -1.0457e-01,  2.6011e-01, -2.7406e-01,  9.3755e-02,  1.0193e-01,\n",
      "         -6.3345e-02,  9.5803e-02],\n",
      "        [ 7.2797e-02, -4.7325e-02, -1.7188e-01,  5.4540e-02, -5.6092e-02,\n",
      "          2.2873e-03,  1.6044e-02, -9.1148e-02, -4.4141e-02, -1.7708e-02,\n",
      "         -7.6761e-02,  6.6334e-02, -1.9239e-02, -1.4807e-01, -3.0697e-02,\n",
      "         -1.7020e-01, -1.4716e-01, -9.6117e-02,  5.7777e-02,  5.1680e-02,\n",
      "          6.5743e-02,  2.6381e-02, -3.1272e-02, -3.4273e-02, -4.5746e-02,\n",
      "         -2.3636e-02,  1.9548e-01, -2.8273e-01,  6.8662e-02,  6.2796e-02,\n",
      "         -1.4054e-02,  3.0684e-02]], grad_fn=<ViewBackward>)\n",
      "output:  tensor([[-0.0161],\n",
      "        [ 0.1759],\n",
      "        [ 0.4652],\n",
      "        [ 0.6975],\n",
      "        [ 0.8038],\n",
      "        [ 0.7843],\n",
      "        [ 0.7797],\n",
      "        [ 0.8533],\n",
      "        [ 0.9702],\n",
      "        [ 1.0406],\n",
      "        [ 1.0239],\n",
      "        [ 0.9461],\n",
      "        [ 0.8561],\n",
      "        [ 0.7798],\n",
      "        [ 0.7073],\n",
      "        [ 0.6115],\n",
      "        [ 0.4760],\n",
      "        [ 0.3086],\n",
      "        [ 0.1339],\n",
      "        [-0.0266]], grad_fn=<AddmmBackward>)\n",
      "Loss:  0.0049586910754442215\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the rnn and monitor results\n",
    "n_steps = 1\n",
    "print_every = 15\n",
    "\n",
    "trained_rnn = train(rnn, n_steps, print_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Series Prediction\n",
    "\n",
    "Time-series prediction can be applied to many tasks. Think about weather forecasting or predicting the ebb and flow of stock market prices. You can even try to generate predictions much further in the future than just one time step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
