{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN\n",
    "\n",
    "In ths notebook, we're going to train a simple RNN to do **time-series prediction**. Given some set of input data, it should be able to generate a prediction for the next time step!\n",
    "<img src='assets/time_prediction.png' width=40% />\n",
    "\n",
    "> * First, we'll create our data\n",
    "* Then, define an RNN in PyTorch\n",
    "* Finally, we'll train our network and see how it performs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import resources and create data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21,)\n",
      "(21, 1)\n",
      "x:  [[0.        ]\n",
      " [0.15643447]\n",
      " [0.30901699]\n",
      " [0.4539905 ]\n",
      " [0.58778525]\n",
      " [0.70710678]\n",
      " [0.80901699]\n",
      " [0.89100652]\n",
      " [0.95105652]\n",
      " [0.98768834]\n",
      " [1.        ]\n",
      " [0.98768834]\n",
      " [0.95105652]\n",
      " [0.89100652]\n",
      " [0.80901699]\n",
      " [0.70710678]\n",
      " [0.58778525]\n",
      " [0.4539905 ]\n",
      " [0.30901699]\n",
      " [0.15643447]] y:  [[1.56434465e-01]\n",
      " [3.09016994e-01]\n",
      " [4.53990500e-01]\n",
      " [5.87785252e-01]\n",
      " [7.07106781e-01]\n",
      " [8.09016994e-01]\n",
      " [8.91006524e-01]\n",
      " [9.51056516e-01]\n",
      " [9.87688341e-01]\n",
      " [1.00000000e+00]\n",
      " [9.87688341e-01]\n",
      " [9.51056516e-01]\n",
      " [8.91006524e-01]\n",
      " [8.09016994e-01]\n",
      " [7.07106781e-01]\n",
      " [5.87785252e-01]\n",
      " [4.53990500e-01]\n",
      " [3.09016994e-01]\n",
      " [1.56434465e-01]\n",
      " [1.22464680e-16]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "# how many time steps/data pts are in one batch of data\n",
    "seq_length = 20\n",
    "\n",
    "# generate evenly spaced data pts\n",
    "time_steps = np.linspace(0, np.pi, seq_length + 1) #0 se pi tk 21 numbers generate krega\n",
    "data = np.sin(time_steps) #sin of each num generated\n",
    "print(data.shape)\n",
    "\n",
    "data.resize((seq_length + 1, 1)) # size becomes (seq_length+1, 1), adds an input_size dimension\n",
    "\n",
    "\n",
    "print(data.shape)\n",
    "x = data[:-1] # all but the last piece of data\n",
    "y = data[1:] # all but the first\n",
    "print('x: ',x,'y: ',y)\n",
    "# display the data\n",
    "plt.plot(time_steps[1:], x, 'r.', label='input, x') # x\n",
    "plt.plot(time_steps[1:], y, 'b.', label='target, y') # y\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define the RNN\n",
    "\n",
    "Next, we define an RNN in PyTorch. We'll use `nn.RNN` to create an RNN layer, then we'll add a last, fully-connected layer to get the output size that we want. An RNN takes in a number of parameters:\n",
    "* **input_size** - the size of the input\n",
    "* **hidden_dim** - the number of features in the RNN output and in the hidden state\n",
    "* **n_layers** - the number of layers that make up the RNN, typically 1-3; greater than 1 means that you'll create a stacked RNN\n",
    "* **batch_first** - whether or not the input/output of the RNN will have the batch_size as the first dimension (batch_size, seq_length, hidden_dim)\n",
    "\n",
    "Take a look at the [RNN documentation](https://pytorch.org/docs/stable/nn.html#rnn) to read more about recurrent layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim=hidden_dim\n",
    "\n",
    "        # define an RNN with specified parameters\n",
    "        # batch_first means that the first dim of the input and output will be the batch_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)#hidden dimension is no. of outputs from each h-state\n",
    "        \n",
    "        # last, fully-connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # x (batch_size, seq_length, input_size)\n",
    "        # hidden (n_layers, batch_size, hidden_dim)\n",
    "        # r_out (batch_size, time_step, hidden_size)\n",
    "        #print(x.shape)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # get RNN outputs\n",
    "        print('shape of x: ',x.shape)\n",
    "        print(\"value in x: \",x)\n",
    "        r_out, hidden = self.rnn(x, hidden)\n",
    "        print(\"Value in r_out: \",r_out)#it will give 10 outputs\n",
    "        print(\"r_out shape1: \",r_out.shape)#[1,20 #means 20 times (seq_length) output dega 10 hrr hidden state se,10]\n",
    "        \n",
    "        print(\"value in hidden: \",hidden)#output at each time step for one batch at a time\n",
    "        print(\"hidden shape1: \",hidden.shape) \n",
    "        \n",
    "        # shape output to be (batch_size*seq_length, hidden_dim)\n",
    "        r_out = r_out.view(-1, self.hidden_dim)  \n",
    "        \n",
    "        print(\"r_out shape2(to be send into fc layer): \",r_out.shape)\n",
    "        print(\"r_out after flattening: \",r_out)\n",
    "        \n",
    "        # get final output \n",
    "        output = self.fc(r_out)\n",
    "        print(\"output: \",output)\n",
    "        \n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the input and output dimensions\n",
    "\n",
    "As a check that your model is working as expected, test out how it responds to input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size:  torch.Size([1, 20, 2])\n",
      "shape of x:  torch.Size([1, 20, 2])\n",
      "value in x:  tensor([[[0.0000e+00, 1.6459e-01],\n",
      "         [3.2470e-01, 4.7595e-01],\n",
      "         [6.1421e-01, 7.3572e-01],\n",
      "         [8.3717e-01, 9.1577e-01],\n",
      "         [9.6940e-01, 9.9658e-01],\n",
      "         [9.9658e-01, 9.6940e-01],\n",
      "         [9.1577e-01, 8.3717e-01],\n",
      "         [7.3572e-01, 6.1421e-01],\n",
      "         [4.7595e-01, 3.2470e-01],\n",
      "         [1.6459e-01, 1.2246e-16],\n",
      "         [0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00]]])\n",
      "Value in r_out:  tensor([[[ 0.0286,  0.1284,  0.0655,  0.0236, -0.5834, -0.1174, -0.1101,\n",
      "          -0.1374,  0.3755,  0.3236],\n",
      "         [-0.1346,  0.0221,  0.2675, -0.0809, -0.3988, -0.1048, -0.3587,\n",
      "          -0.0785,  0.1612,  0.5175],\n",
      "         [-0.2517,  0.0259,  0.3772, -0.0023, -0.4391, -0.0060, -0.3345,\n",
      "          -0.1210,  0.1356,  0.4383],\n",
      "         [-0.3315,  0.0175,  0.4295, -0.0368, -0.4237,  0.0784, -0.3769,\n",
      "          -0.0638,  0.0569,  0.4075],\n",
      "         [-0.3673,  0.0049,  0.4694, -0.0400, -0.4587,  0.1360, -0.3662,\n",
      "          -0.0551,  0.0404,  0.3924],\n",
      "         [-0.3935, -0.0205,  0.4886, -0.0518, -0.4691,  0.1490, -0.3664,\n",
      "          -0.0358,  0.0366,  0.3944],\n",
      "         [-0.4003, -0.0360,  0.4866, -0.0518, -0.4882,  0.1429, -0.3508,\n",
      "          -0.0287,  0.0506,  0.3831],\n",
      "         [-0.3893, -0.0559,  0.4570, -0.0561, -0.5066,  0.1140, -0.3264,\n",
      "          -0.0217,  0.0830,  0.3709],\n",
      "         [-0.3634, -0.0750,  0.4028, -0.0593, -0.5271,  0.0663, -0.2898,\n",
      "          -0.0251,  0.1305,  0.3583],\n",
      "         [-0.3253, -0.0963,  0.3273, -0.0623, -0.5447,  0.0025, -0.2459,\n",
      "          -0.0366,  0.1866,  0.3477],\n",
      "         [-0.2931, -0.0935,  0.2744, -0.0554, -0.5340, -0.0116, -0.2453,\n",
      "          -0.0657,  0.1878,  0.3493],\n",
      "         [-0.2739, -0.0893,  0.2521, -0.0712, -0.5285, -0.0338, -0.2331,\n",
      "          -0.0916,  0.2020,  0.3667],\n",
      "         [-0.2707, -0.0922,  0.2502, -0.0677, -0.5182, -0.0505, -0.2304,\n",
      "          -0.1012,  0.2155,  0.3748],\n",
      "         [-0.2713, -0.0839,  0.2482, -0.0646, -0.5140, -0.0566, -0.2306,\n",
      "          -0.1032,  0.2158,  0.3714],\n",
      "         [-0.2670, -0.0825,  0.2458, -0.0635, -0.5143, -0.0587, -0.2303,\n",
      "          -0.1017,  0.2159,  0.3692],\n",
      "         [-0.2645, -0.0803,  0.2435, -0.0638, -0.5140, -0.0590, -0.2307,\n",
      "          -0.1023,  0.2161,  0.3698],\n",
      "         [-0.2633, -0.0804,  0.2431, -0.0638, -0.5135, -0.0608, -0.2302,\n",
      "          -0.1028,  0.2173,  0.3713],\n",
      "         [-0.2632, -0.0796,  0.2430, -0.0633, -0.5129, -0.0617, -0.2302,\n",
      "          -0.1030,  0.2175,  0.3714],\n",
      "         [-0.2628, -0.0793,  0.2429, -0.0630, -0.5128, -0.0620, -0.2302,\n",
      "          -0.1027,  0.2175,  0.3711],\n",
      "         [-0.2624, -0.0790,  0.2426, -0.0630, -0.5128, -0.0620, -0.2303,\n",
      "          -0.1027,  0.2174,  0.3711]]], grad_fn=<TransposeBackward1>)\n",
      "r_out shape1:  torch.Size([1, 20, 10])\n",
      "value in hidden:  tensor([[[ 0.2763,  0.2868, -0.0046,  0.1881,  0.2342, -0.0870,  0.3503,\n",
      "          -0.2184,  0.0878,  0.1096]],\n",
      "\n",
      "        [[-0.2624, -0.0790,  0.2426, -0.0630, -0.5128, -0.0620, -0.2303,\n",
      "          -0.1027,  0.2174,  0.3711]]], grad_fn=<StackBackward>)\n",
      "hidden shape1:  torch.Size([2, 1, 10])\n",
      "r_out shape2(to be send into fc layer):  torch.Size([20, 10])\n",
      "r_out after flattening:  tensor([[ 0.0286,  0.1284,  0.0655,  0.0236, -0.5834, -0.1174, -0.1101, -0.1374,\n",
      "          0.3755,  0.3236],\n",
      "        [-0.1346,  0.0221,  0.2675, -0.0809, -0.3988, -0.1048, -0.3587, -0.0785,\n",
      "          0.1612,  0.5175],\n",
      "        [-0.2517,  0.0259,  0.3772, -0.0023, -0.4391, -0.0060, -0.3345, -0.1210,\n",
      "          0.1356,  0.4383],\n",
      "        [-0.3315,  0.0175,  0.4295, -0.0368, -0.4237,  0.0784, -0.3769, -0.0638,\n",
      "          0.0569,  0.4075],\n",
      "        [-0.3673,  0.0049,  0.4694, -0.0400, -0.4587,  0.1360, -0.3662, -0.0551,\n",
      "          0.0404,  0.3924],\n",
      "        [-0.3935, -0.0205,  0.4886, -0.0518, -0.4691,  0.1490, -0.3664, -0.0358,\n",
      "          0.0366,  0.3944],\n",
      "        [-0.4003, -0.0360,  0.4866, -0.0518, -0.4882,  0.1429, -0.3508, -0.0287,\n",
      "          0.0506,  0.3831],\n",
      "        [-0.3893, -0.0559,  0.4570, -0.0561, -0.5066,  0.1140, -0.3264, -0.0217,\n",
      "          0.0830,  0.3709],\n",
      "        [-0.3634, -0.0750,  0.4028, -0.0593, -0.5271,  0.0663, -0.2898, -0.0251,\n",
      "          0.1305,  0.3583],\n",
      "        [-0.3253, -0.0963,  0.3273, -0.0623, -0.5447,  0.0025, -0.2459, -0.0366,\n",
      "          0.1866,  0.3477],\n",
      "        [-0.2931, -0.0935,  0.2744, -0.0554, -0.5340, -0.0116, -0.2453, -0.0657,\n",
      "          0.1878,  0.3493],\n",
      "        [-0.2739, -0.0893,  0.2521, -0.0712, -0.5285, -0.0338, -0.2331, -0.0916,\n",
      "          0.2020,  0.3667],\n",
      "        [-0.2707, -0.0922,  0.2502, -0.0677, -0.5182, -0.0505, -0.2304, -0.1012,\n",
      "          0.2155,  0.3748],\n",
      "        [-0.2713, -0.0839,  0.2482, -0.0646, -0.5140, -0.0566, -0.2306, -0.1032,\n",
      "          0.2158,  0.3714],\n",
      "        [-0.2670, -0.0825,  0.2458, -0.0635, -0.5143, -0.0587, -0.2303, -0.1017,\n",
      "          0.2159,  0.3692],\n",
      "        [-0.2645, -0.0803,  0.2435, -0.0638, -0.5140, -0.0590, -0.2307, -0.1023,\n",
      "          0.2161,  0.3698],\n",
      "        [-0.2633, -0.0804,  0.2431, -0.0638, -0.5135, -0.0608, -0.2302, -0.1028,\n",
      "          0.2173,  0.3713],\n",
      "        [-0.2632, -0.0796,  0.2430, -0.0633, -0.5129, -0.0617, -0.2302, -0.1030,\n",
      "          0.2175,  0.3714],\n",
      "        [-0.2628, -0.0793,  0.2429, -0.0630, -0.5128, -0.0620, -0.2302, -0.1027,\n",
      "          0.2175,  0.3711],\n",
      "        [-0.2624, -0.0790,  0.2426, -0.0630, -0.5128, -0.0620, -0.2303, -0.1027,\n",
      "          0.2174,  0.3711]], grad_fn=<ViewBackward>)\n",
      "output:  tensor([[-0.1939],\n",
      "        [-0.1041],\n",
      "        [-0.0735],\n",
      "        [-0.0565],\n",
      "        [-0.0439],\n",
      "        [-0.0365],\n",
      "        [-0.0361],\n",
      "        [-0.0443],\n",
      "        [-0.0590],\n",
      "        [-0.0785],\n",
      "        [-0.0889],\n",
      "        [-0.0951],\n",
      "        [-0.0977],\n",
      "        [-0.0986],\n",
      "        [-0.0995],\n",
      "        [-0.1003],\n",
      "        [-0.1005],\n",
      "        [-0.1005],\n",
      "        [-0.1006],\n",
      "        [-0.1007]], grad_fn=<AddmmBackward>)\n",
      "Output size:  torch.Size([20, 1])\n",
      "Hidden state size:  torch.Size([2, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "# test that dimensions are as expected\n",
    "test_rnn = RNN(input_size=2, output_size=1, hidden_dim=10, n_layers=2) \n",
    "\n",
    "# generate evenly spaced, test data pts\n",
    "time_steps = np.linspace(0, np.pi, seq_length)\n",
    "\n",
    "data = np.sin(time_steps)\n",
    "\n",
    "data.resize((seq_length, 2)) #ek batch mai 2 input honge means batch size=2 hoga and 1 by 1 hum batch feed krenge to the network\n",
    "\n",
    "#here shape of data is (20,2)\n",
    "#print(data)\n",
    "test_input = torch.Tensor(data).unsqueeze(0) # give it a batch_size of 1 as first dimension\n",
    "\n",
    "print('Input size: ', test_input.size())#([1#this 1 is batch size ,20#total data,2#this is input size means 1 batch mai 2 inputs])\n",
    "\n",
    "# test out rnn sizes\n",
    "test_out, test_h = test_rnn(test_input, None)#calling the forward function #None is initial hidden state\n",
    "\n",
    "print('Output size: ', test_out.size())\n",
    "print('Hidden state size: ', test_h.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training the RNN\n",
    "\n",
    "Next, we'll instantiate an RNN with some specified hyperparameters. Then train it over a series of steps, and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide on hyperparameters\n",
    "input_size=1 \n",
    "output_size=1\n",
    "hidden_dim=32\n",
    "n_layers=1\n",
    "\n",
    "# instantiate an RNN\n",
    "rnn = RNN(input_size, output_size, hidden_dim, n_layers)\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Optimization\n",
    "\n",
    "This is a regression problem: can we train an RNN to accurately predict the next data point, given a current data point?\n",
    "\n",
    ">* The data points are coordinate values, so to compare a predicted and ground_truth point, we'll use a regression loss: the mean squared error.\n",
    "* It's typical to use an Adam optimizer for recurrent models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MSE loss and Adam optimizer with a learning rate of 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.01) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the training function\n",
    "\n",
    "This function takes in an rnn, a number of steps to train for, and returns a trained rnn. This function is also responsible for displaying the loss and the predictions, every so often.\n",
    "\n",
    "#### Hidden State\n",
    "\n",
    "Pay close attention to the hidden state, here:\n",
    "* Before looping over a batch of training data, the hidden state is initialized\n",
    "* After a new hidden state is generated by the rnn, we get the latest hidden state, and use that as input to the rnn for the following steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train the RNN\n",
    "def train(rnn, n_steps, print_every):\n",
    "    \n",
    "    # initialize the hidden state\n",
    "    hidden = None      \n",
    "    \n",
    "    for batch_i, step in enumerate(range(n_steps)):\n",
    "        # defining the training data \n",
    "        time_steps = np.linspace(step * np.pi, (step+1)*np.pi, seq_length + 1)\n",
    "        data = np.sin(time_steps)\n",
    "        data.resize((seq_length + 1, 1)) # input_size=1\n",
    "\n",
    "        x = data[:-1]\n",
    "        y = data[1:]\n",
    "        \n",
    "        # convert data into Tensors\n",
    "        x_tensor = torch.Tensor(x).unsqueeze(0) # unsqueeze gives a 1, batch_size dimension\n",
    "        y_tensor = torch.Tensor(y)\n",
    "\n",
    "        # outputs from the rnn\n",
    "        prediction, hidden = rnn(x_tensor, hidden)\n",
    "\n",
    "        ## Representing Memory ##\n",
    "        # make a new variable for hidden and detach the hidden state from its history\n",
    "        # this way, we don't backpropagate through the entire history\n",
    "        hidden = hidden.data\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(prediction, y_tensor)\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # perform backprop and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # display loss and predictions\n",
    "        if batch_i%print_every == 0:        \n",
    "            print('Loss: ', loss.item())\n",
    "            plt.plot(time_steps[1:], x, 'r.') # input\n",
    "            plt.plot(time_steps[1:], prediction.data.numpy().flatten(), 'b.') # predictions\n",
    "            plt.show()\n",
    "    \n",
    "    return rnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the rnn and monitor results\n",
    "n_steps = 75\n",
    "print_every = 15\n",
    "\n",
    "trained_rnn = train(rnn, n_steps, print_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Series Prediction\n",
    "\n",
    "Time-series prediction can be applied to many tasks. Think about weather forecasting or predicting the ebb and flow of stock market prices. You can even try to generate predictions much further in the future than just one time step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
